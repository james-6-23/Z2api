import { serve } from "https://deno.land/std@0.224.0/http/server.ts";

// ç±»å‹å®šä¹‰
interface ChatMessage {
  role: string;
  content: string;
  reasoning_content?: string;
}

interface OpenAIRequest {
  model: string;
  messages: ChatMessage[];
  stream?: boolean;
  temperature?: number;
  max_tokens?: number;
}

interface UpstreamRequest {
  stream: boolean;
  model: string;
  messages: ChatMessage[];
  params: Record<string, any>;
  features: Record<string, any>;
  background_tasks?: Record<string, boolean>;
  chat_id?: string;
  id?: string;
  mcp_servers?: string[];
  model_item?: {
    id: string;
    name: string;
    owned_by: string;
  };
  tool_servers?: string[];
  variables?: Record<string, string>;
}

interface Delta {
  role?: string;
  content?: string;
  reasoning_content?: string;
}

interface Choice {
  index: number;
  delta?: Delta;
  message?: ChatMessage;
  finish_reason?: string;
}

interface OpenAIResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Choice[];
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

interface UpstreamData {
  type: string;
  data: {
    delta_content: string;
    edit_content: string;
    phase: string;
    done: boolean;
    usage?: any;
    error?: {
      detail: string;
      code: number;
    };
    data?: {
      error?: {
        detail: string;
        code: number;
      };
    };
  };
  error?: {
    detail: string;
    code: number;
  };
}

interface Model {
  id: string;
  object: string;
  created: number;
  owned_by: string;
}

// æ—¥å¿—ç³»ç»Ÿç±»å‹å®šä¹‰
type LogLevel = "INFO" | "WARN" | "ERROR";

interface RequestLog {
  request_id: string;
  timestamp: string;
  level: LogLevel;
  type: "request";
  client_ip: string;
  api_key: string;
  model: string;
  messages?: ChatMessage[];
  parameters?: Record<string, any>;
  user_agent?: string;
}

interface ResponseLog {
  request_id: string;
  timestamp: string;
  level: LogLevel;
  type: "response";
  status_code: number;
  response_time_ms: number;
  endpoint: string;
  retry_count: number;
  content?: any;
  reasoning_content?: string;
  error?: string;
}

// é…ç½®å¸¸é‡
const UPSTREAM_URL = "https://chat.z.ai/api/chat/completions";
const PORT = parseInt(Deno.env.get("PORT") || "8080");
const DEFAULT_KEY = Deno.env.get("DEFAULT_KEY") || "123456";
const UPSTREAM_TOKEN = Deno.env.get("UPSTREAM_TOKEN") || "eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6Ijc3NWI4MjMyLTFjMDgtNDZjOC1iM2ZjLTc4NGZkOTYzOTFkMCIsImVtYWlsIjoiR3Vlc3QtMTc1NjQxNzIwODY2NkBndWVzdC5jb20ifQ.ANLFGzTOIhaocgsVRMtzhcHOfhvxWrf3RwiEV0b4mmeNMu72fIbp9j0D42aWlrupZN5AARqGPeIDUFU5po0gFQ";

// æ¨¡å‹é…ç½®
const DEFAULT_MODEL_NAME = "GLM-4.5";
const THINKING_MODEL_NAME = "GLM-4.5-Thinking";
const SEARCH_MODEL_NAME = "GLM-4.5-Search";

// ä¼ªè£…å‰ç«¯å¤´éƒ¨
const X_FE_VERSION = "prod-fe-1.0.70";
const BROWSER_UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0";
const SEC_CH_UA = '"Not;A=Brand";v="99", "Microsoft Edge";v="139", "Chromium";v="139"';
const SEC_CH_UA_MOB = "?0";
const SEC_CH_UA_PLAT = '"Windows"';
const ORIGIN_BASE = "https://chat.z.ai";

// æ€§èƒ½é…ç½®
const PERFORMANCE_MODE = Deno.env.get("PERFORMANCE_MODE") || "balanced";
const ANON_TOKEN_ENABLED = Deno.env.get("ANON_TOKEN_ENABLED") !== "false";
const THINK_TAGS_MODE = Deno.env.get("THINK_TAGS_MODE") || "think";
const DEBUG_MODE = Deno.env.get("DEBUG_MODE") === "true";

// æ ¹æ®æ€§èƒ½æ¨¡å¼è®¾ç½®å‚æ•°
const getPerformanceConfig = () => {
  const mode = PERFORMANCE_MODE.toLowerCase();
  
  switch (mode) {
    case "fast":
      return {
        maxRetries: parseInt(Deno.env.get("MAX_RETRIES") || "1"),
        retryDelay: parseInt(Deno.env.get("RETRY_DELAY") || "200"),
        requestTimeout: parseInt(Deno.env.get("REQUEST_TIMEOUT") || "10000"),
        randomDelayMin: parseInt(Deno.env.get("RANDOM_DELAY_MIN") || "0"),
        randomDelayMax: parseInt(Deno.env.get("RANDOM_DELAY_MAX") || "100")
      };
    case "secure":
      return {
        maxRetries: parseInt(Deno.env.get("MAX_RETRIES") || "5"),
        retryDelay: parseInt(Deno.env.get("RETRY_DELAY") || "2000"),
        requestTimeout: parseInt(Deno.env.get("REQUEST_TIMEOUT") || "60000"),
        randomDelayMin: parseInt(Deno.env.get("RANDOM_DELAY_MIN") || "500"),
        randomDelayMax: parseInt(Deno.env.get("RANDOM_DELAY_MAX") || "1500")
      };
    default: // balanced
      return {
        maxRetries: parseInt(Deno.env.get("MAX_RETRIES") || "3"),
        retryDelay: parseInt(Deno.env.get("RETRY_DELAY") || "1000"),
        requestTimeout: parseInt(Deno.env.get("REQUEST_TIMEOUT") || "30000"),
        randomDelayMin: parseInt(Deno.env.get("RANDOM_DELAY_MIN") || "100"),
        randomDelayMax: parseInt(Deno.env.get("RANDOM_DELAY_MAX") || "500")
      };
  }
};

const config = getPerformanceConfig();
const MAX_RETRIES = config.maxRetries;
const RETRY_DELAY = config.retryDelay;
const REQUEST_TIMEOUT = config.requestTimeout;

// æ—¥å¿—é…ç½®
const ENABLE_DETAILED_LOGGING = Deno.env.get("ENABLE_DETAILED_LOGGING") !== "false";
const LOG_USER_MESSAGES = Deno.env.get("LOG_USER_MESSAGES") === "true";
const LOG_RESPONSE_CONTENT = Deno.env.get("LOG_RESPONSE_CONTENT") === "true";

// æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
const SUPPORTED_MODELS: Model[] = [
  { id: DEFAULT_MODEL_NAME, object: "model", created: Date.now(), owned_by: "z.ai" },
  { id: THINKING_MODEL_NAME, object: "model", created: Date.now(), owned_by: "z.ai" },
  { id: SEARCH_MODEL_NAME, object: "model", created: Date.now(), owned_by: "z.ai" }
];

// æ€§èƒ½ç»Ÿè®¡
let requestCount = 0;
let totalResponseTime = 0;
let errorCount = 0;

// éšæœºå»¶è¿Ÿå‡½æ•°
const randomDelay = () => {
  const min = config.randomDelayMin;
  const max = config.randomDelayMax;
  const delay = Math.random() * (max - min) + min;
  return new Promise(resolve => setTimeout(resolve, delay));
};

// æ—¥å¿—ç³»ç»Ÿå‡½æ•°
function generateRequestId(): string {
  const bytes = new Uint8Array(8);
  crypto.getRandomValues(bytes);
  return `req_${Array.from(bytes, b => b.toString(16).padStart(2, '0')).join('')}`;
}

function maskApiKey(apiKey: string): string {
  if (apiKey.length <= 8) {
    return "*".repeat(apiKey.length);
  }
  return apiKey.slice(0, 4) + "*".repeat(apiKey.length - 8) + apiKey.slice(-4);
}

function getClientIp(request: Request): string {
  const xff = request.headers.get("X-Forwarded-For");
  if (xff) {
    const ips = xff.split(",");
    if (ips.length > 0) {
      return ips[0].trim();
    }
  }

  const xri = request.headers.get("X-Real-IP");
  if (xri) {
    return xri;
  }

  return "unknown";
}

function logStructured(data: any): void {
  if (!ENABLE_DETAILED_LOGGING) {
    return;
  }

  try {
    console.log(JSON.stringify(data));
  } catch (error) {
    console.error("æ—¥å¿—åºåˆ—åŒ–å¤±è´¥:", error);
  }
}

function debugLog(format: string, ...args: any[]): void {
  if (DEBUG_MODE) {
    console.log(`[DEBUG] ${format}`, ...args);
  }
}

function logRequest(
  requestId: string,
  clientIp: string,
  apiKey: string,
  model: string,
  messageCount: number,
  parameters?: Record<string, any>,
  userAgent?: string
): void {
  if (!ENABLE_DETAILED_LOGGING) {
    return;
  }

  const requestLog: RequestLog = {
    request_id: requestId,
    timestamp: new Date().toISOString(),
    level: "INFO",
    type: "request",
    client_ip: clientIp,
    api_key: maskApiKey(apiKey),
    model: model,
    user_agent: userAgent,
  };

  requestLog.parameters = {
    message_count: messageCount,
    parameters: parameters,
  };

  logStructured(requestLog);
}

function logResponse(
  requestId: string,
  statusCode: number,
  responseTime: number,
  endpoint: string,
  retryCount: number,
  error?: string
): void {
  if (!ENABLE_DETAILED_LOGGING) {
    return;
  }

  let level: LogLevel = "INFO";
  if (statusCode >= 400) {
    level = "ERROR";
  } else if (statusCode >= 300) {
    level = "WARN";
  }

  const responseLog: ResponseLog = {
    request_id: requestId,
    timestamp: new Date().toISOString(),
    level: level,
    type: "response",
    status_code: statusCode,
    response_time_ms: responseTime,
    endpoint: endpoint,
    retry_count: retryCount,
    error: error,
  };

  logStructured(responseLog);
}

// è·å–åŒ¿åtokenï¼ˆæ¯æ¬¡å¯¹è¯ä½¿ç”¨ä¸åŒtokenï¼Œé¿å…å…±äº«è®°å¿†ï¼‰
async function getAnonymousToken(): Promise<string> {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), 10000);

  try {
    const response = await fetch(ORIGIN_BASE + "/api/v1/auths/", {
      method: "GET",
      headers: {
        "User-Agent": BROWSER_UA,
        "Accept": "*/*",
        "Accept-Language": "zh-CN,zh;q=0.9",
        "X-FE-Version": X_FE_VERSION,
        "sec-ch-ua": SEC_CH_UA,
        "sec-ch-ua-mobile": SEC_CH_UA_MOB,
        "sec-ch-ua-platform": SEC_CH_UA_PLAT,
        "Origin": ORIGIN_BASE,
        "Referer": ORIGIN_BASE + "/",
      },
      signal: controller.signal
    });

    clearTimeout(timeoutId);

    if (!response.ok) {
      throw new Error(`anon token status=${response.status}`);
    }

    const body = await response.json();
    if (!body.token) {
      throw new Error("anon token empty");
    }

    return body.token;
  } catch (error) {
    clearTimeout(timeoutId);
    throw error;
  }
}

// æ€è€ƒå†…å®¹è½¬æ¢å‡½æ•°
function transformThinking(s: string): string {
  // å» <summary>â€¦</summary>
  s = s.replace(/(?s)<summary>.*?<\/summary>/g, "");
  // æ¸…ç†æ®‹ç•™è‡ªå®šä¹‰æ ‡ç­¾ï¼Œå¦‚ </thinking>ã€<Full> ç­‰
  s = s.replace(/<\/thinking>/g, "");
  s = s.replace(/<Full>/g, "");
  s = s.replace(/<\/Full>/g, "");
  s = s.trim();

  switch (THINK_TAGS_MODE) {
    case "think":
      s = s.replace(/<details[^>]*>/g, "<think>");
      s = s.replace(/<\/details>/g, "</think>");
      break;
    case "strip":
      s = s.replace(/<details[^>]*>/g, "");
      s = s.replace(/<\/details>/g, "");
      break;
  }

  // å¤„ç†æ¯è¡Œå‰ç¼€ "> "ï¼ˆåŒ…æ‹¬èµ·å§‹ä½ç½®ï¼‰
  s = s.replace(/^> /, "");
  s = s.replace(/\n> /g, "\n");
  return s.trim();
}

// å¸¦é‡è¯•çš„è¯·æ±‚å‡½æ•°
async function fetchWithRetry(
  upstreamReq: UpstreamRequest,
  chatID: string,
  authToken: string,
  retries = MAX_RETRIES
): Promise<Response> {
  let lastError: Error | null = null;

  for (let i = 0; i < retries; i++) {
    try {
      // æ·»åŠ å»¶è¿Ÿ
      if (i > 0) {
        await new Promise(resolve => setTimeout(resolve, RETRY_DELAY * Math.pow(2, i)));
      }

      await randomDelay();

      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), REQUEST_TIMEOUT);

      debugLog(`å°è¯•è¯·æ±‚ä¸Šæ¸¸ (ç¬¬${i + 1}æ¬¡å°è¯•)`);

      const response = await fetch(UPSTREAM_URL, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Accept": "application/json, text/event-stream",
          "User-Agent": BROWSER_UA,
          "Authorization": "Bearer " + authToken,
          "Accept-Language": "zh-CN",
          "sec-ch-ua": SEC_CH_UA,
          "sec-ch-ua-mobile": SEC_CH_UA_MOB,
          "sec-ch-ua-platform": SEC_CH_UA_PLAT,
          "X-FE-Version": X_FE_VERSION,
          "Origin": ORIGIN_BASE,
          "Referer": ORIGIN_BASE + "/c/" + chatID,
        },
        body: JSON.stringify(upstreamReq),
        signal: controller.signal
      });

      clearTimeout(timeoutId);

      if (response.ok) {
        debugLog("è¯·æ±‚æˆåŠŸ");
        return response;
      }

      // å¦‚æœæ˜¯é™æµæˆ–å°ç¦é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
      if (response.status === 429 || response.status === 403) {
        const waitTime = Math.min(RETRY_DELAY * Math.pow(2, i), 10000);
        debugLog(`è¢«é™æµæˆ–å°ç¦ (${response.status})ï¼Œç­‰å¾… ${waitTime}ms åé‡è¯•...`);
        await new Promise(resolve => setTimeout(resolve, waitTime));
        continue;
      }

      throw new Error(`HTTP ${response.status}: ${response.statusText}`);

    } catch (error) {
      lastError = error instanceof Error ? error : new Error('æœªçŸ¥é”™è¯¯');
      debugLog(`è¯·æ±‚å°è¯• ${i + 1}/${retries} å¤±è´¥:`, lastError.message);

      if (i === retries - 1) {
        debugLog("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥");
        break;
      }
    }
  }

  throw lastError || new Error('æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥');
}

// ä¸»å¤„ç†å‡½æ•°
async function handler(req: Request): Promise<Response> {
  const url = new URL(req.url);

  // å¥åº·æ£€æŸ¥æ¥å£
  if (req.method === "GET" && url.pathname === "/health") {
    const stats = {
      status: "ok",
      timestamp: new Date().toISOString(),
      performance_mode: PERFORMANCE_MODE,
      config: {
        max_retries: MAX_RETRIES,
        retry_delay: RETRY_DELAY,
        request_timeout: REQUEST_TIMEOUT,
        random_delay: `${config.randomDelayMin}-${config.randomDelayMax}ms`
      },
      stats: {
        total_requests: requestCount,
        average_response_time: requestCount > 0 ? Math.round(totalResponseTime / requestCount) : 0,
        error_rate: requestCount > 0 ? Math.round((errorCount / requestCount) * 100) : 0
      }
    };

    return new Response(JSON.stringify(stats, null, 2), {
      status: 200,
      headers: {
        "Content-Type": "application/json",
        "Access-Control-Allow-Origin": "*",
        "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
        "Access-Control-Allow-Headers": "Content-Type, Authorization"
      }
    });
  }

  // æ¨¡å‹åˆ—è¡¨æ¥å£
  if (req.method === "GET" && url.pathname === "/v1/models") {
    return new Response(JSON.stringify({
      object: "list",
      data: SUPPORTED_MODELS
    }), {
      status: 200,
      headers: {
        "Content-Type": "application/json",
        "Access-Control-Allow-Origin": "*",
        "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
        "Access-Control-Allow-Headers": "Content-Type, Authorization"
      }
    });
  }

  // OPTIONS å¤„ç†
  if (req.method === "OPTIONS") {
    return new Response(null, {
      status: 200,
      headers: {
        "Access-Control-Allow-Origin": "*",
        "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
        "Access-Control-Allow-Headers": "Content-Type, Authorization",
        "Access-Control-Allow-Credentials": "true"
      }
    });
  }

  // èŠå¤©å®Œæˆæ¥å£
  if (req.method === "POST" && url.pathname === "/v1/chat/completions") {
    const startTime = Date.now();
    requestCount++;

    // ç”Ÿæˆè¯·æ±‚ ID å’Œè·å–å®¢æˆ·ç«¯ä¿¡æ¯
    const requestId = generateRequestId();
    const clientIp = getClientIp(req);
    const userAgent = req.headers.get("User-Agent") || "";

    const body = await req.text();

    // API Key éªŒè¯
    const auth = req.headers.get("Authorization");
    const key = auth?.replace("Bearer ", "").trim();
    if (!key || key !== DEFAULT_KEY) {
      const responseTime = Date.now() - startTime;
      logResponse(requestId, 401, responseTime, "", 0, "Unauthorized");
      return new Response(JSON.stringify({ error: "Unauthorized" }), {
        status: 401,
        headers: {
          "Content-Type": "application/json",
          "Access-Control-Allow-Origin": "*"
        }
      });
    }

    debugLog("API keyéªŒè¯é€šè¿‡");

    // è§£æè¯·æ±‚ä½“
    let parsed: OpenAIRequest;
    try {
      parsed = JSON.parse(body) as OpenAIRequest;
    } catch (error) {
      const responseTime = Date.now() - startTime;
      logResponse(requestId, 400, responseTime, "", 0, "Invalid JSON format");
      return new Response(JSON.stringify({ error: "Invalid JSON format" }), {
        status: 400,
        headers: {
          "Content-Type": "application/json",
          "Access-Control-Allow-Origin": "*"
        }
      });
    }

    debugLog(`è¯·æ±‚è§£ææˆåŠŸ - æ¨¡å‹: ${parsed.model}, æµå¼: ${parsed.stream}, æ¶ˆæ¯æ•°: ${parsed.messages.length}`);

    // ç”Ÿæˆä¼šè¯ç›¸å…³ID
    const chatID = `${Date.now()}-${Math.floor(Date.now() / 1000)}`;
    const msgID = `${Date.now()}`;

    let isThinking = false;
    let isSearch = false;
    let searchMcp = "";

    if (parsed.model === THINKING_MODEL_NAME) {
      isThinking = true;
    } else if (parsed.model === SEARCH_MODEL_NAME) {
      isThinking = true;
      isSearch = true;
      searchMcp = "deep-web-search";
    }

    // è®°å½•è¯·æ±‚æ—¥å¿—
    const parameters = {
      stream: parsed.stream,
      temperature: parsed.temperature,
      max_tokens: parsed.max_tokens,
    };
    logRequest(requestId, clientIp, key, parsed.model, parsed.messages.length, parameters, userAgent);

    // æ„é€ ä¸Šæ¸¸è¯·æ±‚
    const upstreamReq: UpstreamRequest = {
      stream: true, // æ€»æ˜¯ä½¿ç”¨æµå¼ä»ä¸Šæ¸¸è·å–
      chat_id: chatID,
      id: msgID,
      model: "0727-360B-API", // ä¸Šæ¸¸å®é™…æ¨¡å‹ID
      messages: parsed.messages,
      params: {},
      features: {
        enable_thinking: isThinking,
        web_search: isSearch,
        auto_web_search: isSearch,
      },
      background_tasks: {
        title_generation: false,
        tags_generation: false,
      },
      mcp_servers: [searchMcp],
      model_item: {
        id: "0727-360B-API",
        name: "GLM-4.5",
        owned_by: "openai"
      },
      tool_servers: [],
      variables: {
        "{{USER_NAME}}": "User",
        "{{USER_LOCATION}}": "Unknown",
        "{{CURRENT_DATETIME}}": new Date().toISOString().replace('T', ' ').slice(0, 19),
      },
    };

    // é€‰æ‹©æœ¬æ¬¡å¯¹è¯ä½¿ç”¨çš„token
    let authToken = UPSTREAM_TOKEN;
    if (ANON_TOKEN_ENABLED) {
      try {
        const t = await getAnonymousToken();
        authToken = t;
        debugLog(`åŒ¿åtokenè·å–æˆåŠŸ: ${t.length > 10 ? t.slice(0, 10) + "..." : t}`);
      } catch (error) {
        debugLog(`åŒ¿åtokenè·å–å¤±è´¥ï¼Œå›é€€å›ºå®štoken: ${error}`);
      }
    }

    const isStream = parsed.stream === true;

    // è¯·æ±‚ä¸Šæ¸¸API
    let response: Response;
    try {
      response = await fetchWithRetry(upstreamReq, chatID, authToken);
    } catch (error) {
      errorCount++;
      const responseTime = Date.now() - startTime;
      totalResponseTime += responseTime;

      const errorMsg = error instanceof Error ? error.message : "æœªçŸ¥é”™è¯¯";
      logResponse(requestId, 502, responseTime, "upstream", MAX_RETRIES, errorMsg);
      console.error('ä¸Šæ¸¸APIè¯·æ±‚å¤±è´¥:', error);
      return new Response(JSON.stringify({
        error: "External API request failed",
        details: errorMsg,
        retry_after: 60,
        performance_mode: PERFORMANCE_MODE
      }), {
        status: 502,
        headers: {
          "Content-Type": "application/json",
          "Access-Control-Allow-Origin": "*"
        }
      });
    }

    // å¤„ç†å“åº”
    if (!isStream) {
      return handleNonStreamResponse(response, requestId, startTime);
    } else {
      return handleStreamResponse(response, requestId, startTime);
    }
  }

  return new Response(JSON.stringify({ error: "Not Found" }), {
    status: 404,
    headers: {
      "Content-Type": "application/json",
      "Access-Control-Allow-Origin": "*"
    }
  });
}

// å¤„ç†éæµå¼å“åº”
async function handleNonStreamResponse(response: Response, requestId: string, startTime: number): Promise<Response> {
  try {
    // æ”¶é›†å®Œæ•´å“åº”
    let fullContent = "";
    const reader = response.body?.getReader();
    if (!reader) {
      throw new Error("æ— æ³•è·å–å“åº”æµ");
    }

    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const chunk = decoder.decode(value);
      const lines = chunk.split("\n");

      for (const line of lines) {
        if (!line.startsWith("data: ")) continue;

        const dataStr = line.slice(6).trim();
        if (dataStr === "" || dataStr === "[DONE]") continue;

        try {
          const upstreamData = JSON.parse(dataStr) as UpstreamData;

          if (upstreamData.data.delta_content) {
            let out = upstreamData.data.delta_content;
            if (upstreamData.data.phase === "thinking") {
              out = transformThinking(out);
            }
            if (out) {
              fullContent += out;
            }
          }

          if (upstreamData.data.done || upstreamData.data.phase === "done") {
            break;
          }
        } catch (parseError) {
          // å¿½ç•¥JSONè§£æé”™è¯¯
        }
      }
    }

    const responseTime = Date.now() - startTime;
    totalResponseTime += responseTime;

    logResponse(requestId, 200, responseTime, "upstream", 0, undefined);
    debugLog(`éæµå¼å“åº”å®Œæˆ: ${responseTime}ms`);

    // æ„é€ å®Œæ•´å“åº”
    const openaiResponse: OpenAIResponse = {
      id: `chatcmpl-${Math.floor(Date.now() / 1000)}`,
      object: "chat.completion",
      created: Math.floor(Date.now() / 1000),
      model: DEFAULT_MODEL_NAME,
      choices: [{
        index: 0,
        message: {
          role: "assistant",
          content: fullContent,
        },
        finish_reason: "stop",
      }],
      usage: {
        prompt_tokens: 0,
        completion_tokens: 0,
        total_tokens: 0,
      },
    };

    return new Response(JSON.stringify(openaiResponse), {
      status: 200,
      headers: {
        "Content-Type": "application/json",
        "Access-Control-Allow-Origin": "*"
      }
    });
  } catch (error) {
    errorCount++;
    const responseTime = Date.now() - startTime;
    totalResponseTime += responseTime;

    const errorMsg = error instanceof Error ? error.message : "å¤„ç†å“åº”å¤±è´¥";
    logResponse(requestId, 500, responseTime, "upstream", 0, errorMsg);

    return new Response(JSON.stringify({ error: "Internal server error" }), {
      status: 500,
      headers: {
        "Content-Type": "application/json",
        "Access-Control-Allow-Origin": "*"
      }
    });
  }
}

// å¤„ç†æµå¼å“åº”
function handleStreamResponse(response: Response, requestId: string, startTime: number): Response {
  const stream = new ReadableStream({
    async start(controller) {
      try {
        const reader = response.body?.getReader();
        if (!reader) {
          controller.close();
          return;
        }

        const decoder = new TextDecoder();
        let isInThinkBlock = false;
        let bufferedThinkContent = "";
        let streamClosed = false;
        let sentInitialAnswer = false;

        // å‘é€ç¬¬ä¸€ä¸ªchunkï¼ˆroleï¼‰
        const firstChunk: OpenAIResponse = {
          id: `chatcmpl-${Math.floor(Date.now() / 1000)}`,
          object: "chat.completion.chunk",
          created: Math.floor(Date.now() / 1000),
          model: DEFAULT_MODEL_NAME,
          choices: [{
            index: 0,
            delta: { role: "assistant" },
          }],
        };

        const firstData = `data: ${JSON.stringify(firstChunk)}\n\n`;
        controller.enqueue(new TextEncoder().encode(firstData));

        while (true) {
          try {
            const { done, value } = await reader.read();
            if (done || streamClosed) break;

            const chunk = decoder.decode(value);
            const lines = chunk.split("\n");

            for (const line of lines) {
              if (streamClosed) break;

              if (line.startsWith("data: ")) {
                const jsonText = line.slice(6).trim();

                if (jsonText === "[DONE]") {
                  // å‘é€ç¼“å­˜çš„æ€è€ƒå†…å®¹
                  if (isInThinkBlock && bufferedThinkContent) {
                    try {
                      const thinkChunk: OpenAIResponse = {
                        id: `chatcmpl-${Math.floor(Date.now() / 1000)}`,
                        object: "chat.completion.chunk",
                        created: Math.floor(Date.now() / 1000),
                        model: DEFAULT_MODEL_NAME,
                        choices: [{
                          index: 0,
                          delta: { content: `<think>${bufferedThinkContent}</think>` },
                        }],
                      };
                      const output = `data: ${JSON.stringify(thinkChunk)}\n\n`;
                      controller.enqueue(new TextEncoder().encode(output));
                    } catch (e) {
                      console.warn('å‘é€æ€è€ƒå†…å®¹å¤±è´¥:', e);
                    }
                  }

                  try {
                    controller.enqueue(new TextEncoder().encode("data: [DONE]\n\n"));
                  } catch (e) {
                    console.warn('å‘é€ç»“æŸæ ‡è®°å¤±è´¥:', e);
                  }
                  streamClosed = true;
                  break;
                }

                if (jsonText) {
                  try {
                    const upstreamData = JSON.parse(jsonText) as UpstreamData;

                    // é”™è¯¯æ£€æµ‹
                    if (upstreamData.error || upstreamData.data.error ||
                        (upstreamData.data.data && upstreamData.data.data.error)) {
                      debugLog("ä¸Šæ¸¸é”™è¯¯ï¼Œç»“æŸæµ");
                      const endChunk: OpenAIResponse = {
                        id: `chatcmpl-${Math.floor(Date.now() / 1000)}`,
                        object: "chat.completion.chunk",
                        created: Math.floor(Date.now() / 1000),
                        model: DEFAULT_MODEL_NAME,
                        choices: [{ index: 0, delta: {}, finish_reason: "stop" }],
                      };
                      const endData = `data: ${JSON.stringify(endChunk)}\n\n`;
                      controller.enqueue(new TextEncoder().encode(endData));
                      controller.enqueue(new TextEncoder().encode("data: [DONE]\n\n"));
                      streamClosed = true;
                      break;
                    }

                    // å¤„ç†EditContentåœ¨æœ€åˆçš„answerä¿¡æ¯ï¼ˆåªå‘é€ä¸€æ¬¡ï¼‰
                    if (!sentInitialAnswer && upstreamData.data.edit_content &&
                        upstreamData.data.phase === "answer") {
                      const out = upstreamData.data.edit_content;
                      if (out) {
                        const parts = out.split("</details>");
                        if (parts.length > 1) {
                          const content = parts[1];
                          if (content) {
                            debugLog("å‘é€åˆå§‹ç­”æ¡ˆå†…å®¹");
                            const chunk: OpenAIResponse = {
                              id: `chatcmpl-${Math.floor(Date.now() / 1000)}`,
                              object: "chat.completion.chunk",
                              created: Math.floor(Date.now() / 1000),
                              model: DEFAULT_MODEL_NAME,
                              choices: [{ index: 0, delta: { content: content } }],
                            };
                            const chunkData = `data: ${JSON.stringify(chunk)}\n\n`;
                            controller.enqueue(new TextEncoder().encode(chunkData));
                            sentInitialAnswer = true;
                          }
                        }
                      }
                    }

                    if (upstreamData.data.delta_content) {
                      let out = upstreamData.data.delta_content;
                      if (upstreamData.data.phase === "thinking") {
                        out = transformThinking(out);
                        // æ€è€ƒå†…å®¹ä½¿ç”¨ reasoning_content å­—æ®µ
                        if (out) {
                          debugLog("å‘é€æ€è€ƒå†…å®¹");
                          const chunk: OpenAIResponse = {
                            id: `chatcmpl-${Math.floor(Date.now() / 1000)}`,
                            object: "chat.completion.chunk",
                            created: Math.floor(Date.now() / 1000),
                            model: DEFAULT_MODEL_NAME,
                            choices: [{
                              index: 0,
                              delta: { reasoning_content: out },
                            }],
                          };
                          const chunkData = `data: ${JSON.stringify(chunk)}\n\n`;
                          controller.enqueue(new TextEncoder().encode(chunkData));
                        }
                      } else {
                        // æ™®é€šå†…å®¹ä½¿ç”¨ content å­—æ®µ
                        if (out) {
                          debugLog("å‘é€æ™®é€šå†…å®¹");
                          const chunk: OpenAIResponse = {
                            id: `chatcmpl-${Math.floor(Date.now() / 1000)}`,
                            object: "chat.completion.chunk",
                            created: Math.floor(Date.now() / 1000),
                            model: DEFAULT_MODEL_NAME,
                            choices: [{
                              index: 0,
                              delta: { content: out },
                            }],
                          };
                          const chunkData = `data: ${JSON.stringify(chunk)}\n\n`;
                          controller.enqueue(new TextEncoder().encode(chunkData));
                        }
                      }
                    }

                    // æ£€æŸ¥æ˜¯å¦ç»“æŸ
                    if (upstreamData.data.done || upstreamData.data.phase === "done") {
                      debugLog("æ£€æµ‹åˆ°æµç»“æŸä¿¡å·");
                      // å‘é€ç»“æŸchunk
                      const endChunk: OpenAIResponse = {
                        id: `chatcmpl-${Math.floor(Date.now() / 1000)}`,
                        object: "chat.completion.chunk",
                        created: Math.floor(Date.now() / 1000),
                        model: DEFAULT_MODEL_NAME,
                        choices: [{
                          index: 0,
                          delta: {},
                          finish_reason: "stop",
                        }],
                      };
                      const endData = `data: ${JSON.stringify(endChunk)}\n\n`;
                      controller.enqueue(new TextEncoder().encode(endData));

                      // å‘é€[DONE]
                      controller.enqueue(new TextEncoder().encode("data: [DONE]\n\n"));
                      debugLog("æµå¼å“åº”å®Œæˆ");
                      streamClosed = true;
                      break;
                    }
                  } catch (parseError) {
                    // å¿½ç•¥ JSON è§£æé”™è¯¯
                  }
                }
              }
            }
          } catch (readError) {
            console.warn('è¯»å–æ•°æ®å¤±è´¥:', readError);
            streamClosed = true;
            break;
          }
        }
      } catch (error) {
        console.error('æµå¤„ç†é”™è¯¯:', error);
      } finally {
        try {
          controller.close();
        } catch (closeError) {
          // å¿½ç•¥å…³é—­é”™è¯¯
        }
      }
    }
  });

  return new Response(stream, {
    status: 200,
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      "Connection": "keep-alive",
      "Access-Control-Allow-Origin": "*",
      "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
      "Access-Control-Allow-Headers": "Content-Type, Authorization"
    }
  });
}

// å¯åŠ¨æœåŠ¡å™¨
console.log(`ğŸš€ Z2API Denoä¼˜åŒ–ç‰ˆå¯åŠ¨åœ¨ç«¯å£ ${PORT}`);
console.log(`âš¡ æ€§èƒ½æ¨¡å¼: ${PERFORMANCE_MODE}`);
console.log(`ğŸ”§ é…ç½®: retries=${MAX_RETRIES}, delay=${RETRY_DELAY}ms, timeout=${REQUEST_TIMEOUT}ms`);
console.log(`â±ï¸  éšæœºå»¶è¿Ÿ: ${config.randomDelayMin}-${config.randomDelayMax}ms`);
console.log(`ğŸ“ è¯¦ç»†æ—¥å¿—: ${ENABLE_DETAILED_LOGGING}, ç”¨æˆ·æ¶ˆæ¯: ${LOG_USER_MESSAGES}, å“åº”å†…å®¹: ${LOG_RESPONSE_CONTENT}`);
console.log(`ğŸ” åŒ¿åtoken: ${ANON_TOKEN_ENABLED}, æ€è€ƒæ ‡ç­¾æ¨¡å¼: ${THINK_TAGS_MODE}`);
console.log(`ğŸ¯ æ”¯æŒæ¨¡å‹: ${SUPPORTED_MODELS.map(m => m.id).join(", ")}`);

serve(handler, { port: PORT });
